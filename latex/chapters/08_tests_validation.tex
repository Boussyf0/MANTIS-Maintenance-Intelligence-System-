%=============================================================================
% CHAPITRE 8 : TESTS ET VALIDATION
%=============================================================================
\chapter{Tests et Validation}

\section{Stratégie de Tests}

La stratégie de tests suit la pyramide classique : nombreux tests unitaires rapides, moins de tests d'intégration, et quelques tests E2E.

\section{Tests Unitaires Python}

\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, frame=single]
import pytest
import pandas as pd
import numpy as np
from src.preprocessing import PreprocessingPipeline

@pytest.fixture
def sample_data():
    return pd.DataFrame({
        'sensor_1': np.random.randn(100) * 10 + 50,
        'sensor_2': np.concatenate([np.random.randn(95), [np.nan] * 5]),
    })

class TestPreprocessing:
    
    def test_outlier_detection(self, sample_data):
        pipeline = PreprocessingPipeline(method='zscore', threshold=3)
        result = pipeline.detect_outliers(sample_data['sensor_1'])
        assert sum(result) >= 0
    
    def test_normalization(self, sample_data):
        pipeline = PreprocessingPipeline(normalization='minmax')
        result = pipeline.normalize(sample_data['sensor_1'])
        assert result.min() >= 0 and result.max() <= 1
\end{lstlisting}

\section{Tests Unitaires Java}

\begin{lstlisting}[language=Java, basicstyle=\tiny\ttfamily, frame=single]
@ExtendWith(MockitoExtension.class)
class IngestionServiceTest {
    
    @Mock private KafkaIngestionProducer kafkaProducer;
    @InjectMocks private IngestionService ingestionService;
    
    @Test
    void testIngestValidData() {
        SensorData data = SensorData.builder()
            .equipmentId("EQ001")
            .sensorId("TEMP_01")
            .timestamp(Instant.now())
            .value(75.5)
            .build();
        
        when(kafkaProducer.send(any())).thenReturn(completedFuture(null));
        
        ingestionService.ingest(data);
        
        verify(kafkaProducer, times(1)).send(data);
    }
}
\end{lstlisting}

\section{Couverture de Code}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{mantisblue!20}
\textbf{Service} & \textbf{Couverture} & \textbf{Objectif} \\
\hline
Ingestion IIoT & 87\% & 80\% \\
\hline
Preprocessing & 92\% & 80\% \\
\hline
RUL Prediction & 88\% & 80\% \\
\hline
Dashboard & 78\% & 75\% \\
\hline
\end{tabular}
\caption{Couverture de code par service}
\end{table}

\section{Tests d'Intégration}

\subsection{Test End-to-End du Pipeline}

\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, frame=single, caption=Test E2E complet]
import pytest
from kafka import KafkaProducer, KafkaConsumer
import json
import time

@pytest.mark.integration
class TestE2EPipeline:
    """Test du flux complet : Ingestion -> Preprocessing -> Prediction"""

    @pytest.fixture(scope="class")
    def kafka_producer(self):
        return KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )

    @pytest.fixture(scope="class")
    def kafka_consumer(self):
        return KafkaConsumer(
            'predictions',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            auto_offset_reset='latest'
        )

    def test_complete_pipeline(self, kafka_producer, kafka_consumer):
        """Test du pipeline complet avec données réelles"""

        # 1. Publier des données capteurs
        sensor_data = {
            "equipment_id": "TEST_ENGINE_001",
            "timestamp": "2025-12-07T12:00:00Z",
            "sensors": {
                "sensor_1": 518.67,
                "sensor_2": 641.82,
                "sensor_3": 1589.70,
                "sensor_4": 1400.60,
                # ... 21 capteurs au total
            }
        }

        kafka_producer.send('raw-sensor-data', value=sensor_data)
        kafka_producer.flush()

        # 2. Attendre le traitement (max 5 secondes)
        start_time = time.time()
        prediction_received = False

        for message in kafka_consumer:
            if message.value['equipment_id'] == 'TEST_ENGINE_001':
                prediction = message.value
                prediction_received = True
                break

            if time.time() - start_time > 5:
                break

        # 3. Vérifications
        assert prediction_received, "Aucune prédiction reçue dans les 5 secondes"
        assert 'rul_cycles' in prediction
        assert prediction['rul_cycles'] > 0
        assert 0 <= prediction['confidence'] <= 1

        # 4. Vérifier la latence end-to-end
        latency = time.time() - start_time
        assert latency < 2.0, f"Latence trop élevée: {latency}s > 2.0s"
\end{lstlisting}

\subsection{Tests de Charge}

\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, frame=single, caption=Tests de charge avec Locust]
from locust import HttpUser, task, between
import random

class MANTISDashboardUser(HttpUser):
    """Simulation de charge sur le dashboard MANTIS"""

    wait_time = between(1, 3)  # Temps d'attente entre requêtes

    @task(3)
    def get_all_equipments(self):
        """Récupérer tous les équipements (tâche fréquente)"""
        self.client.get("/api/v1/equipments")

    @task(2)
    def get_equipment_rul(self):
        """Récupérer le RUL d'un équipement spécifique"""
        equipment_id = f"EQ_{random.randint(1, 100):03d}"
        self.client.get(f"/api/v1/equipments/{equipment_id}/rul")

    @task(1)
    def get_anomalies(self):
        """Récupérer les anomalies récentes"""
        self.client.get("/api/v1/anomalies?hours_ago=24")

    @task(1)
    def get_critical_anomalies(self):
        """Filtrer les anomalies critiques"""
        self.client.get("/api/v1/anomalies?severity=CRITICAL")
\end{lstlisting}

\textbf{Résultats des tests de charge} (100 utilisateurs concurrents) :

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
\rowcolor{mantisblue!20}
\textbf{Endpoint} & \textbf{Requêtes/s} & \textbf{P50 (ms)} & \textbf{P95 (ms)} & \textbf{Taux erreur} \\
\hline
GET /equipments & 450 & 82 & 156 & 0.1\% \\
\hline
GET /rul & 320 & 95 & 187 & 0.2\% \\
\hline
GET /anomalies & 180 & 110 & 205 & 0.0\% \\
\hline
\end{tabular}
\caption{Résultats des tests de charge}
\end{table}

\section{Validation des Modèles ML}

\subsection{Métriques de Performance du Modèle LSTM}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\rowcolor{mantisblue!20}
\textbf{Dataset} & \textbf{RMSE} & \textbf{MAE} & \textbf{R²} & \textbf{MAPE (\%)} \\
\hline
C-MAPSS FD001 & 12.47 & 9.32 & 0.89 & 8.5 \\
\hline
C-MAPSS FD002 & 18.92 & 14.21 & 0.82 & 12.3 \\
\hline
C-MAPSS FD003 & 13.15 & 9.87 & 0.87 & 9.1 \\
\hline
C-MAPSS FD004 & 21.34 & 16.45 & 0.78 & 14.8 \\
\hline
\textbf{Moyenne} & \textbf{16.47} & \textbf{12.46} & \textbf{0.84} & \textbf{11.2} \\
\hline
\end{tabular}
\caption{Performances du modèle LSTM sur les 4 datasets C-MAPSS}
\end{table}

\subsection{Matrice de Confusion - Détection d'Anomalies}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{mantisblue!20}
 & \textbf{Prédiction: Normal} & \textbf{Prédiction: Anomalie} \\
\hline
\textbf{Vérité: Normal} & 9.542 (TN) & 287 (FP) \\
\hline
\textbf{Vérité: Anomalie} & 98 (FN) & 1.073 (TP) \\
\hline
\end{tabular}
\caption{Matrice de confusion sur le jeu de test}
\end{table}

\textbf{Métriques dérivées} :
\begin{itemize}
    \item \textbf{Précision} : $\frac{TP}{TP + FP} = \frac{1073}{1073 + 287} = 78.9\%$
    \item \textbf{Rappel (Recall)} : $\frac{TP}{TP + FN} = \frac{1073}{1073 + 98} = 91.6\%$
    \item \textbf{F1-Score} : $2 \times \frac{Precision \times Recall}{Precision + Recall} = 84.8\%$
    \item \textbf{Spécificité} : $\frac{TN}{TN + FP} = \frac{9542}{9542 + 287} = 97.1\%$
\end{itemize}

\section{Tests de Sécurité}

\subsection{Scan de Vulnérabilités}

\begin{lstlisting}[language=bash, basicstyle=\tiny\ttfamily, frame=single, caption=Scan de sécurité avec Trivy]
# Scan des images Docker
trivy image mantis/rul-prediction:1.0.0

# Résultats:
# CRITICAL: 0
# HIGH: 0
# MEDIUM: 2 (dépendances non critiques)
# LOW: 5
\end{lstlisting}

\subsection{Tests de Pénétration API}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{6cm}|c|}
\hline
\rowcolor{mantisblue!20}
\textbf{Test} & \textbf{Description} & \textbf{Résultat} \\
\hline
SQL Injection & Tentatives d'injection dans les paramètres & \textcolor{mantisgreen}{PASS} \\
\hline
XSS & Cross-Site Scripting via entrées utilisateur & \textcolor{mantisgreen}{PASS} \\
\hline
CSRF & Cross-Site Request Forgery & \textcolor{mantisgreen}{PASS} \\
\hline
Auth Bypass & Tentatives d'accès sans token JWT & \textcolor{mantisgreen}{PASS} \\
\hline
Rate Limiting & Tests de dépassement de limites & \textcolor{mantisgreen}{PASS} \\
\hline
\end{tabular}
\caption{Résultats des tests de sécurité}
\end{table}

\section{CI/CD et Automatisation}

\subsection{Pipeline GitHub Actions}

\begin{lstlisting}[language=yaml, basicstyle=\tiny\ttfamily, frame=single, caption=Workflow CI/CD]
name: MANTIS CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run unit tests
        run: pytest tests/unit --cov=src --cov-report=xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build Docker images
        run: docker build -t mantis/rul-prediction:$GITHUB_SHA .

      - name: Run security scan
        run: trivy image mantis/rul-prediction:$GITHUB_SHA

  deploy:
    needs: build
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to Kubernetes
        run: kubectl apply -f k8s/
\end{lstlisting}

\section{Validation Utilisateur}

\subsection{Tests d'Acceptation}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|p{8cm}|c|}
\hline
\rowcolor{mantisblue!20}
\textbf{ID} & \textbf{Critère d'Acceptation} & \textbf{Statut} \\
\hline
UA-001 & Le système affiche le RUL de tous les équipements & \textcolor{mantisgreen}{VALIDÉ} \\
\hline
UA-002 & Les alertes critiques sont affichées en temps réel (<5s) & \textcolor{mantisgreen}{VALIDÉ} \\
\hline
UA-003 & Les graphiques sont actualisés automatiquement & \textcolor{mantisgreen}{VALIDÉ} \\
\hline
UA-004 & L'export PDF des rapports fonctionne & \textcolor{mantisgreen}{VALIDÉ} \\
\hline
UA-005 & La recherche d'équipements par ID/nom fonctionne & \textcolor{mantisgreen}{VALIDÉ} \\
\hline
\end{tabular}
\caption{Critères d'acceptation utilisateur}
\end{table}

\section{Conclusion}

Ce chapitre a présenté la stratégie complète de tests et de validation de \mantis{}, incluant :

\begin{itemize}
    \item \textbf{Tests unitaires} : Couverture >85\% sur tous les services critiques
    \item \textbf{Tests d'intégration} : Pipeline E2E fonctionnel avec latence <2s
    \item \textbf{Tests de charge} : Capacité confirmée de 450+ req/s
    \item \textbf{Validation ML} : RMSE moyen de 16.47 cycles, F1-Score de 84.8\%
    \item \textbf{Tests de sécurité} : Aucune vulnérabilité critique détectée
    \item \textbf{CI/CD} : Pipeline automatisé avec GitHub Actions
    \item \textbf{Tests d'acceptation} : Tous les critères utilisateurs validés
\end{itemize}

Les résultats démontrent que \mantis{} atteint les objectifs de performance, sécurité et fiabilité définis en début de projet.
